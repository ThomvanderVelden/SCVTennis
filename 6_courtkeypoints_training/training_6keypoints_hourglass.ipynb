{
 "metadata": {
  "colab": {
   "toc_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 8690496,
     "sourceType": "datasetVersion",
     "datasetId": 5211047
    },
    {
     "sourceId": 65350,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 54517
    },
    {
     "sourceId": 65386,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 54549
    }
   ],
   "dockerImageVersionId": 30732,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Start code"
   ],
   "metadata": {
    "id": "16k_O3L1hEeA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "id": "xGeiB20VhEeD",
    "execution": {
     "iopub.status.busy": "2024-06-14T13:33:03.098867Z",
     "iopub.execute_input": "2024-06-14T13:33:03.099840Z",
     "iopub.status.idle": "2024-06-14T13:33:03.105909Z",
     "shell.execute_reply.started": "2024-06-14T13:33:03.099796Z",
     "shell.execute_reply": "2024-06-14T13:33:03.104762Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T16:13:10.167773Z",
     "start_time": "2024-06-14T16:13:10.152248300Z"
    }
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "id": "FrGRHNE5hEeD",
    "execution": {
     "iopub.status.busy": "2024-06-14T13:33:05.170264Z",
     "iopub.execute_input": "2024-06-14T13:33:05.171243Z",
     "iopub.status.idle": "2024-06-14T13:33:05.176747Z",
     "shell.execute_reply.started": "2024-06-14T13:33:05.171205Z",
     "shell.execute_reply": "2024-06-14T13:33:05.175619Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T16:10:49.581214500Z",
     "start_time": "2024-06-14T16:10:49.557047400Z"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create model from scratch (Hourglass)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class HourglassBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(HourglassBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        x1 = F.relu(self.bn2(self.conv2(x1)))\n",
    "        x2 = self.maxpool(x1)\n",
    "        return x1, x2\n",
    "\n",
    "class Hourglass(nn.Module):\n",
    "    def __init__(self, depth, in_channels, out_channels):\n",
    "        super(Hourglass, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.hg_blocks = self._make_hourglass_blocks(depth, in_channels, out_channels)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def _make_hourglass_blocks(self, depth, in_channels, out_channels):\n",
    "        hg_blocks = []\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                hg_blocks.append(HourglassBlock(in_channels, out_channels))\n",
    "            else:\n",
    "                hg_blocks.append(HourglassBlock(out_channels, out_channels))\n",
    "        return nn.ModuleList(hg_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        saved_outputs = []\n",
    "        for i in range(self.depth):\n",
    "            x1, x = self.hg_blocks[i](x)\n",
    "            saved_outputs.append(x1)\n",
    "        for i in range(self.depth - 1, -1, -1):\n",
    "            x = self.upsample(x)\n",
    "            x = x + saved_outputs[i]\n",
    "        return x\n",
    "\n",
    "class HourglassNet(nn.Module):\n",
    "    def __init__(self, num_keypoints, depth=4):\n",
    "        super(HourglassNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.hg = Hourglass(depth, 3, 256)\n",
    "        self.conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.conv2 = nn.Conv2d(256, num_keypoints * 2, kernel_size=1, stride=1, padding=0)  # Output channels = num_keypoints * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hg(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten the output to [batch_size, num_keypoints * 2]\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "G9ZXmJslhEeJ",
    "execution": {
     "iopub.status.busy": "2024-06-14T11:23:12.737271Z",
     "iopub.execute_input": "2024-06-14T11:23:12.737910Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T16:33:39.059645600Z",
     "start_time": "2024-06-14T16:33:39.019513700Z"
    }
   },
   "execution_count": 66,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset preparation and preprosseing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Function to split data into training and validation sets\n",
    "def split_data(image_folder, annotations_path, train_ratio=0.8):\n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    train_annotations, val_annotations = train_test_split(annotations, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    # Save the splits into separate JSON files in a writable directory\n",
    "    # writable_dir = '/kaggle/working'\n",
    "    writable_dir = 'training/own_dataset_6kp/'\n",
    "    train_annotations_path = os.path.join(writable_dir, 'data_train.json')\n",
    "    val_annotations_path = os.path.join(writable_dir, 'data_val.json')\n",
    "\n",
    "    with open(train_annotations_path, 'w') as f:\n",
    "        json.dump(train_annotations, f, indent=4)\n",
    "\n",
    "    with open(val_annotations_path, 'w') as f:\n",
    "        json.dump(val_annotations, f, indent=4)\n",
    "\n",
    "    print(f\"Train annotations saved to: {train_annotations_path}\")\n",
    "    print(f\"Validation annotations saved to: {val_annotations_path}\")\n",
    "\n",
    "# Dataset class for loading images and annotations\n",
    "class KeypointsDataset(Dataset):\n",
    "    def __init__(self, image_folder, annotations_path, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.annotations = self.load_annotations(annotations_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_annotations(self, annotations_path):\n",
    "        with open(annotations_path, 'r') as f:\n",
    "            annotations = json.load(f)\n",
    "        return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        img_path = os.path.join(self.image_folder, annotation['id'] + '.png')\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        keypoints = annotation['kps']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image, 'keypoints': torch.tensor(keypoints, dtype=torch.float32)}\n",
    "        return sample\n",
    "\n",
    "# Function to create DataLoader instances\n",
    "def create_dataloaders(data_folder_path, batch_size=8, kaggle=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    if kaggle:\n",
    "        train_dataset = KeypointsDataset(os.path.join(data_folder_path, 'images'), '/kaggle/working/data_train.json', transform=transform)\n",
    "        val_dataset = KeypointsDataset(os.path.join(data_folder_path, 'images'), '/kaggle/working/data_val.json', transform=transforms.ToTensor())\n",
    "    else:\n",
    "        train_dataset = KeypointsDataset(data_folder_path, 'training/own_dataset_6kp/data_train.json', transform=transform)\n",
    "        val_dataset = KeypointsDataset(data_folder_path, 'training/own_dataset_6kp/data_val.json', transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T16:33:41.534273700Z",
     "start_time": "2024-06-14T16:33:41.484191900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "id": "pFywhbpthEeK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "image_folder = 'training/own_dataset_6kp'\n",
    "annotations_path = os.path.join(image_folder, 'annotations.json')\n",
    "split_data(image_folder, annotations_path)\n",
    "\n",
    "data_folder_path = 'training/own_dataset_6kp'\n",
    "batch_size = 4\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(data_folder_path, batch_size=batch_size, kaggle=False)\n"
   ],
   "metadata": {
    "id": "C2bFdi5yhEeK",
    "ExecuteTime": {
     "end_time": "2024-06-14T16:33:45.029703400Z",
     "start_time": "2024-06-14T16:33:44.999148200Z"
    }
   },
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train annotations saved to: training/own_dataset_6kp/data_train.json\n",
      "Validation annotations saved to: training/own_dataset_6kp/data_val.json\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load Hourglass Network model\n",
    "num_keypoints = 6 * 2  # 6 keypoints with (x, y) coordinates\n",
    "model = HourglassNet(num_keypoints=num_keypoints, depth=4)\n"
   ],
   "metadata": {
    "id": "KRnbEv7JhEeK",
    "ExecuteTime": {
     "end_time": "2024-06-14T16:33:47.260729400Z",
     "start_time": "2024-06-14T16:33:47.219477900Z"
    }
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# Set initial learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn9\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T16:33:48.565676400Z",
     "start_time": "2024-06-14T16:33:48.545629700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1204224) must match the size of tensor b (12) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[71], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m      9\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(images)\n\u001B[1;32m---> 10\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeypoints\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\Desktop\\SVCV\\TennisProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\SVCV\\TennisProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\SVCV\\TennisProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    534\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\SVCV\\TennisProject\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3365\u001B[0m, in \u001B[0;36mmse_loss\u001B[1;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3363\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3365\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3366\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[1;32m~\\Desktop\\SVCV\\TennisProject\\venv\\lib\\site-packages\\torch\\functional.py:76\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[1;34m(*tensors)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (1204224) must match the size of tensor b (12) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        images = batch['image']\n",
    "        keypoints = batch['keypoints'].view(batch['keypoints'].size(0), -1)  # Flatten the target keypoints\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, keypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image']\n",
    "            keypoints = batch['keypoints'].view(batch['keypoints'].size(0), -1)  # Flatten the target keypoints\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, keypoints)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    running_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T16:33:57.855277400Z",
     "start_time": "2024-06-14T16:33:50.673022600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
